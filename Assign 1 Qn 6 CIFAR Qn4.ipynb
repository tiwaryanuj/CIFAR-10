{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  59.,   43.,   50., ...,   84.,   72.,    1.],\n",
       "        [ 154.,  126.,  105., ...,  142.,  144.,    1.],\n",
       "        [ 255.,  253.,  253., ...,   83.,   84.,    1.],\n",
       "        ..., \n",
       "        [  35.,   40.,   42., ...,   66.,   50.,    1.],\n",
       "        [ 189.,  186.,  185., ...,  171.,  171.,    1.],\n",
       "        [ 229.,  236.,  234., ...,  162.,  161.,    1.]]),\n",
       " array([6, 9, 9, ..., 9, 1, 1]),\n",
       " array([[ 158.,  159.,  165., ...,  129.,  110.,    1.],\n",
       "        [ 235.,  231.,  232., ...,  191.,  199.,    1.],\n",
       "        [ 158.,  158.,  139., ...,    3.,    7.,    1.],\n",
       "        ..., \n",
       "        [  20.,   19.,   15., ...,   53.,   47.,    1.],\n",
       "        [  25.,   15.,   23., ...,   81.,   80.,    1.],\n",
       "        [  73.,   98.,   99., ...,   58.,   26.,    1.]]),\n",
       " array([3, 8, 8, ..., 5, 1, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "import _pickle\n",
    "\n",
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = _pickle.load(fo,encoding='iso-8859-1')\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def load_data():\n",
    "    X = []\n",
    "    Y = []\n",
    "    for j in range(5):\n",
    "        d = unpickle('cifar-10-batches-py/data_batch_'+str(j+1))\n",
    "        xt = d['data']\n",
    "        y_train1 = d['labels']\n",
    "        X.append(xt)\n",
    "        Y.append(y_train1)\n",
    "\n",
    "        xt = np.concatenate(X)\n",
    "        y_train1 = np.concatenate(Y).astype(np.int32)\n",
    "\n",
    "        d = unpickle('cifar-10-batches-py/test_batch')\n",
    "        x_test = d['data']\n",
    "        y_test = np.array(d['labels'],dtype=np.int32)\n",
    "\n",
    "        X_t = np.reshape(xt, (xt.shape[0], -1))\n",
    "        X_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "\n",
    "        # add bias dimension and transform into columns\n",
    "        X_t = np.hstack([X_t, np.ones((X_t.shape[0], 1))])\n",
    "        X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "       \n",
    "    return X_t, y_train1, X_test, y_test\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    X = []\n",
    "    Y = []\n",
    "    for j in range(5):\n",
    "        d = unpickle('cifar-10-batches-py/data_batch_'+str(j+1))\n",
    "        xt = d['data']\n",
    "        y_train1 = d['labels']\n",
    "        X.append(xt)\n",
    "        Y.append(y_train1)\n",
    "\n",
    "        xt = np.concatenate(X)\n",
    "        y_train1 = np.concatenate(Y).astype(np.int32)\n",
    "\n",
    "        d = unpickle('cifar-10-batches-py/test_batch')\n",
    "        x_test = d['data']\n",
    "        y_test = np.array(d['labels'],dtype=np.int32)\n",
    "\n",
    "        X_t = np.reshape(xt, (xt.shape[0], -1))\n",
    "        X_test = np.reshape(x_test, (x_test.shape[0], -1))\n",
    "\n",
    "\n",
    "        # add bias dimension and transform into columns\n",
    "        X_t = np.hstack([X_t, np.ones((X_t.shape[0], 1))])\n",
    "        X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3073)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3073)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_val, y_train, y_val = cross_validation.train_test_split(X_t, y_train1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3073)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3073)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = 3073 # dimensionality\n",
    "K = 10 # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial values from hyperparameter\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "step_size = 1e-0\n",
    "\n",
    "# Xtt=X_train[:20000,:]\n",
    "# ytt=y_train1[:20000,:]\n",
    "#Xtt=X_train[20001:40000,:]\n",
    "#ytt=y_train[20001:40000,:]\n",
    "\n",
    "Xtt=X_train[:1000,:]\n",
    "ytt=y_train[:1000,]\n",
    "num_examples = Xtt.shape[0]\n",
    "# gradient descent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0  loss: nan\n",
      "iteration: 10  loss: nan\n",
      "iteration: 20  loss: nan\n",
      "iteration: 30  loss: nan\n",
      "iteration: 40  loss: nan\n",
      "iteration: 50  loss: nan\n",
      "iteration: 60  loss: nan\n",
      "iteration: 70  loss: nan\n",
      "iteration: 80  loss: nan\n",
      "iteration: 90  loss: nan\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    scores = (np.dot(Xtt, W) + b )\n",
    "  \n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = (exp_scores / np.sum(exp_scores, axis=1, keepdims=True))/1000 # [N x K]\n",
    "\n",
    "  \n",
    "#     # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),ytt])\n",
    "#     corect_logprobs = -np.log(probs[np.random.randint(0,X_train.shape[0],num_examples),y_train1])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration:\",i, \" loss:\",loss)\n",
    "       \n",
    "  \n",
    "#     # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),ytt] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(Xtt.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
